{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True,  transform=transforms.ToTensor(), download=True)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [50000, 10000])\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(dataset=val_dataset,   batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 6, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(128, 1, 28, 28)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "x = conv1(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3456])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(128, 6, 24, 24)\n",
    "torch.flatten(x, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CNNを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))  # 16, 5, 5\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)  # 256 -> 64\n",
    "        x = self.fc2(x)  # 64 -> 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "images, labels = next(iter(train_loader))\n",
    "preds = model(images)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. しょぼしょぼCNNを学習させてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.7913, Val Loss: 0.6478, Val Acc: 0.8174\n",
      "Epoch: 2, Train Loss: 0.4280, Val Loss: 0.3253, Val Acc: 0.9042\n",
      "Epoch: 3, Train Loss: 0.2880, Val Loss: 0.2415, Val Acc: 0.9295\n",
      "Epoch: 4, Train Loss: 0.2316, Val Loss: 0.2150, Val Acc: 0.9355\n",
      "Epoch: 5, Train Loss: 0.1965, Val Loss: 0.1861, Val Acc: 0.9442\n",
      "Epoch: 6, Train Loss: 0.1712, Val Loss: 0.1601, Val Acc: 0.9532\n",
      "Epoch: 7, Train Loss: 0.1535, Val Loss: 0.1426, Val Acc: 0.9592\n",
      "Epoch: 8, Train Loss: 0.1401, Val Loss: 0.1346, Val Acc: 0.9616\n",
      "Epoch: 9, Train Loss: 0.1295, Val Loss: 0.1306, Val Acc: 0.9621\n",
      "Epoch: 10, Train Loss: 0.1212, Val Loss: 0.1166, Val Acc: 0.9669\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(images)\n",
    "        \n",
    "        loss = criterion(preds, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_loss = []\n",
    "    acc_list = []\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "        acc = accuracy_score(labels, preds.argmax(dim=1))\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1}, '\\\n",
    "          + f'Train Loss: {sum(train_loss)/len(train_loss):.4f}, '\\\n",
    "          + f'Val Loss: {sum(val_loss)/len(val_loss):.4f}, '\n",
    "          + f'Val Acc: {sum(acc_list)/len(acc_list):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Epoch: 1, Train Loss: 0.6841, Val Loss: 0.3671, Val Acc: 0.8958\n",
    "Epoch: 2, Train Loss: 0.3554, Val Loss: 0.3280, Val Acc: 0.9065\n",
    "Epoch: 3, Train Loss: 0.3311, Val Loss: 0.3205, Val Acc: 0.9069\n",
    "Epoch: 4, Train Loss: 0.3189, Val Loss: 0.3071, Val Acc: 0.9104\n",
    "Epoch: 5, Train Loss: 0.3111, Val Loss: 0.3044, Val Acc: 0.9127\n",
    "Epoch: 6, Train Loss: 0.3056, Val Loss: 0.2959, Val Acc: 0.9133\n",
    "Epoch: 7, Train Loss: 0.3011, Val Loss: 0.2945, Val Acc: 0.9117\n",
    "Epoch: 8, Train Loss: 0.2968, Val Loss: 0.2902, Val Acc: 0.9158\n",
    "Epoch: 9, Train Loss: 0.2935, Val Loss: 0.2965, Val Acc: 0.9126\n",
    "Epoch: 10, Train Loss: 0.2911, Val Loss: 0.2948, Val Acc: 0.9129\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
